{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Working with Hugging Face\n",
                "\n",
                "## Getting Started with Hugging Face\n",
                "\n",
                "### Searching the Hub with Python\n",
                "\n",
                "The Hugging Face Hub provides a nice user interface for searching for\n",
                "models and learning more about them. At times, you may find it\n",
                "convenient to be able to do the same thing without leaving the\n",
                "development environment. Fortunately, Hugging Face also provides a\n",
                "Python package which allows you to find models through code.\n",
                "\n",
                "Use the `huggingface_hub` package to find the most downloaded model for\n",
                "text classification.\n",
                "\n",
                "`HfApi` and `ModelFilter` from the `huggingface_hub` package is already\n",
                "loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the instance of the API to the Hugging Face Hub.\n",
                "- Return a list of one item which is the most downloaded model text\n",
                "  classification task.\n",
                "- Store the returned object as a list named `modelList`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Collecting huggingface-hub\n",
                        "  Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
                        "\u001b[K     |████████████████████████████████| 401 kB 2.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from huggingface-hub) (4.11.0)\n",
                        "Collecting tqdm>=4.42.1\n",
                        "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
                        "\u001b[K     |████████████████████████████████| 78 kB 3.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting fsspec>=2023.5.0\n",
                        "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
                        "\u001b[K     |████████████████████████████████| 176 kB 2.3 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting pyyaml>=5.1\n",
                        "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
                        "\u001b[K     |████████████████████████████████| 174 kB 3.5 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from huggingface-hub) (24.0)\n",
                        "Collecting filelock\n",
                        "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
                        "Collecting requests\n",
                        "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
                        "\u001b[K     |████████████████████████████████| 64 kB 6.3 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
                        "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
                        "\u001b[K     |████████████████████████████████| 120 kB 8.0 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting certifi>=2017.4.17\n",
                        "  Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
                        "\u001b[K     |████████████████████████████████| 164 kB 8.1 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting idna<4,>=2.5\n",
                        "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
                        "\u001b[K     |████████████████████████████████| 66 kB 9.6 MB/s  eta 0:00:01\n",
                        "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
                        "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
                        "\u001b[K     |████████████████████████████████| 121 kB 4.2 MB/s eta 0:00:01\n",
                        "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, fsspec, filelock, huggingface-hub\n",
                        "Successfully installed certifi-2024.6.2 charset-normalizer-3.3.2 filelock-3.14.0 fsspec-2024.6.0 huggingface-hub-0.23.2 idna-3.7 pyyaml-6.0.1 requests-2.32.3 tqdm-4.66.4 urllib3-2.2.1\n",
                        "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
                        "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install huggingface-hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/datttrian/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
                        "  warnings.warn(\n",
                        "/Users/datttrian/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "# added/edited\n",
                "from huggingface_hub import HfApi, ModelFilter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/datttrian/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/endpoint_helpers.py:247: FutureWarning: 'ModelFilter' is deprecated and will be removed in huggingface_hub>=0.24. Please pass the filter parameters as keyword arguments directly to the `list_models` method.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "# Create the instance of the API\n",
                "api = HfApi()\n",
                "\n",
                "# Return the filtered list from the Hub\n",
                "models = api.list_models(\n",
                "    filter=ModelFilter(task=\"text-classification\"),\n",
                "    sort=\"downloads\",\n",
                "    direction=-1,\n",
                "  \tlimit=1\n",
                ")\n",
                "\n",
                "# Store as a list\n",
                "modelList = list(models)\n",
                "\n",
                "print(modelList[0].modelId)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Saving a model\n",
                "\n",
                "There may be situations where downloading and storing a model locally\n",
                "(i.e. a directory on your computer) is desired. For example, if working\n",
                "offline.\n",
                "\n",
                "Practice downloading and saving here. An instance of `AutoModel` is\n",
                "already loaded for you under the same name.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Instantiate the model class for the\n",
                "  `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
                "- Save the model as the `modelId` under \"models/\".\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Collecting torch\n",
                        "  Downloading torch-2.3.0-cp39-none-macosx_11_0_arm64.whl (61.0 MB)\n",
                        "\u001b[K     |████████████████████████████████| 61.0 MB 17.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting transformers\n",
                        "  Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
                        "\u001b[K     |████████████████████████████████| 9.1 MB 34.6 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: fsspec in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from torch) (2024.6.0)\n",
                        "Collecting networkx\n",
                        "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
                        "\u001b[K     |████████████████████████████████| 1.6 MB 20.1 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from torch) (4.11.0)\n",
                        "Collecting jinja2\n",
                        "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
                        "\u001b[K     |████████████████████████████████| 133 kB 25.4 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting sympy\n",
                        "  Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
                        "\u001b[K     |████████████████████████████████| 5.7 MB 47.2 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: filelock in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from torch) (3.14.0)\n",
                        "Collecting numpy>=1.17\n",
                        "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
                        "\u001b[K     |████████████████████████████████| 14.0 MB 31.7 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from transformers) (0.23.2)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
                        "Requirement already satisfied: requests in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
                        "Collecting regex!=2019.12.17\n",
                        "  Downloading regex-2024.5.15-cp39-cp39-macosx_11_0_arm64.whl (278 kB)\n",
                        "\u001b[K     |████████████████████████████████| 278 kB 15.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
                        "  Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl (2.4 MB)\n",
                        "\u001b[K     |████████████████████████████████| 2.4 MB 19.9 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting safetensors>=0.4.1\n",
                        "  Downloading safetensors-0.4.3-cp39-cp39-macosx_11_0_arm64.whl (411 kB)\n",
                        "\u001b[K     |████████████████████████████████| 411 kB 39.5 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from transformers) (24.0)\n",
                        "Collecting MarkupSafe>=2.0\n",
                        "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl (18 kB)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.2.1)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.7)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2024.6.2)\n",
                        "Collecting mpmath<1.4.0,>=1.1.0\n",
                        "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
                        "\u001b[K     |████████████████████████████████| 536 kB 37.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hInstalling collected packages: mpmath, MarkupSafe, tokenizers, sympy, safetensors, regex, numpy, networkx, jinja2, transformers, torch\n",
                        "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 regex-2024.5.15 safetensors-0.4.3 sympy-1.12.1 tokenizers-0.19.1 torch-2.3.0 transformers-4.41.2\n",
                        "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
                        "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install torch transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "from transformers import AutoModel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "modelId = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "\n",
                "# Instantiate the AutoModel class\n",
                "model = AutoModel.from_pretrained(modelId)\n",
                "\n",
                "# Save the model\n",
                "model.save_pretrained(save_directory=f\"models/{modelId}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inspecting datasets\n",
                "\n",
                "The datasets on Hugging Face range in terms of size, information, and\n",
                "features. Therefore it's beneficial to inspect it before committing to\n",
                "loading a dataset into your environment.\n",
                "\n",
                "Let's inspect the \"derenrich/wikidata-en-descriptions-small\" dataset.\n",
                "\n",
                "*Note: this exercise may take a minute due to the dataset size.*\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `load_dataset_builder`.\n",
                "- Create the dataset builder to inspect the dataset.\n",
                "- Print the features for the dataset.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Collecting datasets\n",
                        "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
                        "\u001b[K     |████████████████████████████████| 542 kB 3.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (0.23.2)\n",
                        "Collecting fsspec[http]<=2024.3.1,>=2023.1.0\n",
                        "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
                        "\u001b[K     |████████████████████████████████| 171 kB 25.5 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting xxhash\n",
                        "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
                        "Collecting pyarrow>=12.0.0\n",
                        "  Downloading pyarrow-16.1.0-cp39-cp39-macosx_11_0_arm64.whl (26.0 MB)\n",
                        "\u001b[K     |████████████████████████████████| 26.0 MB 13.1 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting pandas\n",
                        "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
                        "\u001b[K     |████████████████████████████████| 11.3 MB 18.1 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting aiohttp\n",
                        "  Downloading aiohttp-3.9.5-cp39-cp39-macosx_11_0_arm64.whl (390 kB)\n",
                        "\u001b[K     |████████████████████████████████| 390 kB 43.1 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: packaging in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (24.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n",
                        "Collecting pyarrow-hotfix\n",
                        "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
                        "Requirement already satisfied: tqdm>=4.62.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (4.66.4)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.1)\n",
                        "Collecting multiprocess\n",
                        "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
                        "\u001b[K     |████████████████████████████████| 133 kB 65.7 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
                        "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
                        "\u001b[K     |████████████████████████████████| 116 kB 36.3 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: filelock in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (3.14.0)\n",
                        "Requirement already satisfied: requests>=2.32.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
                        "Collecting attrs>=17.3.0\n",
                        "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
                        "\u001b[K     |████████████████████████████████| 60 kB 20.6 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
                        "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
                        "Collecting frozenlist>=1.1.1\n",
                        "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
                        "\u001b[K     |████████████████████████████████| 53 kB 17.2 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
                        "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
                        "Collecting aiosignal>=1.1.2\n",
                        "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
                        "Collecting yarl<2.0,>=1.0\n",
                        "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
                        "\u001b[K     |████████████████████████████████| 81 kB 30.9 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.1->datasets) (3.7)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.1->datasets) (2.2.1)\n",
                        "Collecting tzdata>=2022.7\n",
                        "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
                        "\u001b[K     |████████████████████████████████| 345 kB 52.7 MB/s eta 0:00:01\n",
                        "\u001b[?25hCollecting pytz>=2020.1\n",
                        "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
                        "\u001b[K     |████████████████████████████████| 505 kB 61.2 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/datttrian/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
                        "Installing collected packages: multidict, frozenlist, yarl, attrs, async-timeout, aiosignal, tzdata, pytz, fsspec, dill, aiohttp, xxhash, pyarrow-hotfix, pyarrow, pandas, multiprocess, datasets\n",
                        "  Attempting uninstall: fsspec\n",
                        "    Found existing installation: fsspec 2024.6.0\n",
                        "    Uninstalling fsspec-2024.6.0:\n",
                        "      Successfully uninstalled fsspec-2024.6.0\n",
                        "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.19.2 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.3.1 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
                        "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
                        "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading readme: 100%|██████████| 615/615 [00:00<00:00, 1.62MB/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'output': Value(dtype='string', id=None), 'qid': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n"
                    ]
                }
            ],
            "source": [
                "# Load the module\n",
                "from datasets import load_dataset_builder\n",
                "\n",
                "# Create the dataset builder\n",
                "reviews_builder = load_dataset_builder(\"derenrich/wikidata-en-descriptions-small\")\n",
                "\n",
                "# Print the features\n",
                "print(reviews_builder.info.features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading datasets\n",
                "\n",
                "Hugging Face built the `dataset` package for interacting with datasets.\n",
                "There are a lot of convenient functions, including\n",
                "`load_dataset_builder` which we just used. After inspecting a dataset to\n",
                "ensure its the right one for your project, it's time to load the\n",
                "dataset! For this, we can leverage input parameters for `load_dataset`\n",
                "to specify which parts of a dataset to load, i.e. the \"train\" dataset\n",
                "for English wikipedia articles.\n",
                "\n",
                "The `load_dataset` module from the `datasets` package is already loaded\n",
                "for you. Note: the `load_dataset` function was modified for the purpose\n",
                "of this exercise.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Load the `\"wikimedia/wikipedia\"` dataset and save as `wikipedia`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "from datasets import load_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading readme: 100%|██████████| 131k/131k [00:00<00:00, 4.37MB/s]\n",
                        "Downloading data: 100%|██████████| 41/41 [07:37<00:00, 11.15s/files]\n",
                        "Generating train split:  14%|█▍        | 889445/6407814 [00:06<00:37, 145282.16 examples/s]\n"
                    ]
                },
                {
                    "ename": "DatasetGenerationError",
                    "evalue": "An error occurred while generating the dataset",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/builder.py:2011\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_writer.py:590\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/error.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/fsspec/implementations/local.py:389\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the train portion of the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wikipedia \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikimedia/wikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20231101.en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# added/edited\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of the dataset is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wikipedia)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/load.py:2614\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2614\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2623\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2624\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2625\u001b[0m )\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/builder.py:1122\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1129\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/builder.py:1882\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1880\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1883\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1884\u001b[0m     ):\n\u001b[1;32m   1885\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1886\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
                        "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/builder.py:2038\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
                        "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
                    ]
                }
            ],
            "source": [
                "# Load the train portion of the dataset\n",
                "wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")  # added/edited\n",
                "\n",
                "print(f\"The length of the dataset is {len(wikipedia)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Manipulating datasets\n",
                "\n",
                "There will likely be many occasions when you will need to manipulate a\n",
                "dataset before using it within a ML task. Two common manipulations are\n",
                "filtering and selecting (or slicing). Given the size of these datasets,\n",
                "Hugging Face leverages arrow file types.\n",
                "\n",
                "This means performing manipulations are slightly different than what you\n",
                "might be used to. Fortunately, there's already methods to help with\n",
                "this!\n",
                "\n",
                "The dataset is already loaded for you under `wikipedia`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Filter the dataset for rows with the term \"football\" in the `text`\n",
                "  column and save as `filtered`.\n",
                "- Select a single example from the filtered dataset and save as\n",
                "  `example`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter the documents\n",
                "filtered = wikipedia.filter(lambda row: \"football\" in row[\"text\"])\n",
                "\n",
                "# Create a sample dataset\n",
                "example = filtered.select(range(1))\n",
                "\n",
                "print(example[0][\"text\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building Pipelines with Hugging Face\n",
                "\n",
                "### Getting started with pipelines\n",
                "\n",
                "Hugging Face has an ecosystem of libraries that allow users to leverage\n",
                "tools at different levels. The `pipeline` module from the `transformers`\n",
                "library is a great place to get started with performing ML tasks. It\n",
                "removes the requirement for training models, allowing for quicker\n",
                "experimentation and results. It does this by being a wrapper around\n",
                "underlying objects, functions, and processes.\n",
                "\n",
                "Getting started with pipeline can be done by defining a task or model.\n",
                "This helps with quick experimentation as you become familiar with the\n",
                "library.\n",
                "\n",
                "Create your first pipelines for sentiment analysis. The `input` is a\n",
                "sentence string that is already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pipeline` from the `transformers` library.\n",
                "- Create the first pipeline by specifying the task \"sentiment-analysis\"\n",
                "  and save as `task_pipeline`.\n",
                "- Create another pipeline but only specify the model,\n",
                "  `distilbert-base-uncased-finetuned-sst-2-english` and save as\n",
                "  `model_pipeline`.\n",
                "- Predict the sentiment of `input` using both pipelines.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pipeline\n",
                "from transformers import pipeline\n",
                "\n",
                "# Create the task pipeline\n",
                "task_pipeline = pipeline(task=\"sentiment-analysis\")\n",
                "\n",
                "# Create the model pipeline\n",
                "model_pipeline = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "# Predict the sentiment\n",
                "task_output = task_pipeline(input)\n",
                "model_output = model_pipeline(input)\n",
                "\n",
                "print(f\"Sentiment from task_pipeline: {task_output[0]['label']}; Sentiment from model_pipeline: {model_output[0]['label']}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using AutoClasses\n",
                "\n",
                "AutoClasses offer more control for machine learning tasks, and they can\n",
                "also be used with `pipeline()` for quick application. It's a nice\n",
                "balance of control and convenience.\n",
                "\n",
                "Continue with the sentiment analysis task and combine AutoClasses with\n",
                "the pipeline module.\n",
                "\n",
                "`AutoModelForSequenceClassification` and `AutoTokenizer` from the\n",
                "`transformers` library have already been imported for you and the input\n",
                "text is saved as `input`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Download the model and tokenizer for\n",
                "  `\"distilbert-base-uncased-finetuned-sst-2-english\"` and save as\n",
                "  `model` and `tokenizer`, respectively.\n",
                "- Create the pipeline using `model` and `tokenizer` and save as\n",
                "  `sentimentAnalysis`.\n",
                "- Predict the output using `sentimentAnalysis` and save as `output`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the model and tokenizer\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "# Create the pipeline\n",
                "sentimentAnalysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
                "\n",
                "# Predict the sentiment\n",
                "output = sentimentAnalysis(input)\n",
                "\n",
                "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Comparing models with the pipeline\n",
                "\n",
                "One of the great benefits of the `pipeline()` module is the ease at\n",
                "which you can experiment with different models simply by changing the\n",
                "`\"model\"` input. This is a good way to determine which model works best\n",
                "for a particular task or dataset that you are working with.\n",
                "\n",
                "Experiment with two sentiment analysis models by creating pipelines for\n",
                "each, then using them to predict the sentiment for a sentence.\n",
                "\n",
                "`pipeline` from the `transformers` library is already loaded for you.\n",
                "The example input sentence is saved as `input`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a pipeline for labeling text as positive or negative, using the\n",
                "  model `\"distilbert-base-uncased-finetuned-sst-2-english\"`, and save as\n",
                "  `distil_pipeline`.\n",
                "- Predict the sentiment for the `input` and save as `distil_output`.\n",
                "- Repeat the same steps for the model, `\"kwang123/bert-sentiment-analysis\"` and save as `bert_pipeline` and `bert_output`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the pipeline\n",
                "distil_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "# Predict the sentiment\n",
                "distil_output = distil_pipeline(input)\n",
                "\n",
                "\n",
                "# Create the pipeline\n",
                "distil_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "# Predict the sentiment\n",
                "distil_output = distil_pipeline(input)\n",
                "\n",
                "# Create the second pipeline and predict the sentiment\n",
                "bert_pipeline = pipeline(task=\"sentiment-analysis\", model=\"kwang123/bert-sentiment-analysis\")\n",
                "bert_output = bert_pipeline(input)\n",
                "\n",
                "print(f\"Bert Output: {bert_output[0]['label']}\")\n",
                "print(f\"Distil Output: {distil_output[0]['label']}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Normalizing text\n",
                "\n",
                "An important step to performing an NLP task is tokenizing the input\n",
                "text. This makes the text more understandable and manageable for the ML\n",
                "models, or other algorithms.\n",
                "\n",
                "Before performing tokenization, it's best to run normalization steps,\n",
                "i.e. removing white spaces and accents, lowercasing, and more. Each\n",
                "tokenizer available in Hugging Face uses it's own normalization and\n",
                "tokenization processes.\n",
                "\n",
                "Let's take a look at what normalization the `distilbert-base-uncased`\n",
                "tokenizer applies to the `input_string`, \"HOWDY, how aré yoü?\".\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `AutoTokenizer` from `transformers`.\n",
                "- Download the tokenizer for `distilbert-base-uncased` using\n",
                "  `AutoTokenizer` and save as `tokenizer`.\n",
                "- Run the normalization process for the `tokenizer`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the AutoTokenizer\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Download the tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "\n",
                "# Normalize the input string\n",
                "output = tokenizer.backend_tokenizer.normalizer.normalize_str(input_string)\n",
                "\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Comparing tokenizer output\n",
                "\n",
                "Most models in Hugging Face will have an associated tokenizer that will\n",
                "help prepare the input data based on what the model expects. After\n",
                "normalization, the tokenizer will split the input into smaller chunks\n",
                "based on the chosen algorithm. This is known as \"pre-tokenization\".\n",
                "\n",
                "Let's explore the different types of pre-tokenization by performing this\n",
                "process with two tokenizers on the same input. We will be using\n",
                "`DistilBertTokenizer` and `GPT2Tokenizer` which have already been loaded\n",
                "for you. The input text string, \"Pineapple on pizza is pretty good, I\n",
                "guess\" is saved as `input`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Download the `\"gpt2\"` tokenizer and save as `gpt_tokenizer`.\n",
                "- Use `gpt_tokenizer` to create the tokens from the `input`.\n",
                "- Repeat the same two steps for\n",
                "  `\"distilbert-base-uncased-finetuned-sst-2-english\"`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the gpt tokenizer\n",
                "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# Tokenize the input\n",
                "gpt_tokens = gpt_tokenizer.tokenize(text=input)\n",
                "\n",
                "# Repeat for distilbert\n",
                "distil_tokenizer = DistilBertTokenizer.from_pretrained(\n",
                "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                ")\n",
                "distil_tokens = distil_tokenizer.tokenize(text=input)\n",
                "\n",
                "# Compare the output\n",
                "print(f\"GPT tokenizer: {gpt_tokens}\")\n",
                "print(f\"DistilBERT tokenizer: {distil_tokens}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Grammatical correctness\n",
                "\n",
                "Text classification is the process of labeling an input text into a\n",
                "pre-defined category. This can take the form of sentiment - `positive`\n",
                "or `negative` - spam detection - `spam` or `not spam` - and even\n",
                "grammatical errors.\n",
                "\n",
                "Explore the use of a `text-classification` pipeline for checking an\n",
                "input sentence for grammatical errors.\n",
                "\n",
                "`pipeline` from the `transformers` library is already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a pipeline for the task `text-classification` and use the model\n",
                "  `\"abdulmatinomotoso/English_Grammar_Checker\"`, saving the pipeline as\n",
                "  `classifier`.\n",
                "- Use the `classifier` to predict the grammatical correctness of the\n",
                "  input sentence provided and save as `output`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a pipeline\n",
                "classifier = pipeline(\n",
                "    task=\"text-classification\",\n",
                "  model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
                ")\n",
                "\n",
                "# Predict classification\n",
                "output = classifier(\"I will walk dog\")\n",
                "\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Question Natural Language Inference\n",
                "\n",
                "Another task under the text classification umbrella is Question Natural\n",
                "Language Inference, or QNLI. This determines if a piece of text contains\n",
                "enough information to answer a posed question. This requires the model\n",
                "to perform logical reasoning which are important for Q&A applications.\n",
                "\n",
                "Performing different tasks with the `text-classification` pipeline can\n",
                "be done by choosing different models. Each model is trained to predict\n",
                "specific labels and optimized for learning different context within a\n",
                "text.\n",
                "\n",
                "`pipeline` from the `transformers` library is already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a text classification QNLI pipeline using the model\n",
                "  `\"cross-encoder/qnli-electra-base\"` and save as `classifier`.\n",
                "- Use this classifier to predict if the text contains information to\n",
                "  answer the question.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the pipeline\n",
                "classifier = pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
                "\n",
                "# Predict the output\n",
                "output = classifier(\"Where is the capital of France?, Brittany is known for their kouign-amann.\")\n",
                "\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Zero-shot classification\n",
                "\n",
                "Zero-shot classification is the ability for a transformer to predict a\n",
                "label from a new set of classes which it wasn't originally trained to\n",
                "identify. This is possible through its transfer learning capabilities.\n",
                "It can be an extremely valuable tool.\n",
                "\n",
                "Hugging Face `pipeline()` also has a `zero-shot-classification` task.\n",
                "These pipelines require both an input text and candidate labels.\n",
                "\n",
                "Build a zero-shot classifier to predict the label for the input `text`,\n",
                "a news headline that has been loaded for you.\n",
                "\n",
                "`pipelines` from the `transformers` library is already loaded for you.\n",
                "Note that we are using our own version of the pipeline function to\n",
                "enable you to learn how to use these functions without having to\n",
                "download the model.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Build the pipeline for a zero-shot-classification task and save as\n",
                "  `classifier`.\n",
                "- Create a list of the labels - \"politics\", \"science\", \"sports\" - and\n",
                "  save as `candidate_labels`.\n",
                "- Predict the label of `text` using the classifier and candidate labels.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the zero-shot classifier\n",
                "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
                "\n",
                "# Create the list\n",
                "candidate_labels = [\"politics\", \"science\", \"sports\"]\n",
                "\n",
                "# Predict the output\n",
                "output = classifier(text, candidate_labels)\n",
                "\n",
                "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summarizing long text\n",
                "\n",
                "Summarization is a useful task for reducing large piece of text into\n",
                "something more manageable. This could be beneficial for multiple reasons\n",
                "like reducing the amount of time a reader needs to spend to obtain the\n",
                "important point of a piece of text.\n",
                "\n",
                "The Hugging Face `pipeline()` task, \"summarization\", builds a s\n",
                "summarization pipeline which is a quick way to perform summarization on\n",
                "a piece of text. You'll do that by creating a new pipeline and using it\n",
                "to summarize a piece of text from a Wikipedia page on Greece.\n",
                "\n",
                "`pipeline` from the `transformers` library and the `original_text` have\n",
                "already been loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the summarization `pipeline` using the task \"summarization\" and\n",
                "  save as `summarizer`.\n",
                "- Use the new pipeline to create a summary of the text and save as\n",
                "  `summary_text`.\n",
                "- Compare the length of the original and summary text.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the summarization pipeline\n",
                "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
                "\n",
                "# Summarize the text\n",
                "summary_text = summarizer(original_text)\n",
                "\n",
                "# Compare the length\n",
                "print(f\"Original text length: {len(original_text)}\")\n",
                "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using min_length and max_length\n",
                "\n",
                "The `pipeline()` function, has two important parameters: `min_length`\n",
                "and `max_length`. These are useful for adjusting the length of the\n",
                "resulting summary text to be short, longer, or within a certain number\n",
                "of words. You might want to do this if there are space constraints\n",
                "(i.e., small storage), to enhance readability, or improve the quality of\n",
                "the summary.\n",
                "\n",
                "You'll experiment with a short and long summarizer by setting these two\n",
                "parameters to a small range, then a wider range.\n",
                "\n",
                "`pipeline` from the `transformers` library and the `original_text` have\n",
                "already been loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a summarization pipeline using a `min_length` of 1 and\n",
                "  `max_length` of 10; save as `short_summarizer`.\n",
                "- Summarize the `original_text` using the `short_summarizer` and save\n",
                "  the result as `short_summary_text`.\n",
                "- Repeat these steps for a summarization pipeline that has a minimum length of 50 and maximum of 150; save as `long_summarizer` and `long_summary_text`, respectively.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a short summarizer\n",
                "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
                "\n",
                "# Summarize the input text\n",
                "short_summary_text = short_summarizer(original_text)\n",
                "\n",
                "# Print the short summary\n",
                "print(short_summary_text[0][\"summary_text\"])\n",
                "\n",
                "\n",
                "# Create a short summarizer\n",
                "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
                "\n",
                "# Summarize the input text\n",
                "short_summary_text = short_summarizer(original_text)\n",
                "\n",
                "# Print the short summary\n",
                "print(short_summary_text[0][\"summary_text\"])\n",
                "\n",
                "# Repeat for a long summarizer\n",
                "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=50, max_length=150)\n",
                "\n",
                "long_summary_text = long_summarizer(original_text)\n",
                "\n",
                "# Print the long summary\n",
                "print(long_summary_text[0][\"summary_text\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summarizing several inputs\n",
                "\n",
                "Often times, you'll be working on projects where summarization will\n",
                "occur over an entire dataset or list of items, not just a single piece\n",
                "of text. Fortunately, this can be done by passing in a list of text\n",
                "items. This will return a list of summarized texts.\n",
                "\n",
                "You'll build a final summarization pipeline and use it to summarize a\n",
                "list of text items from the `wiki` dataset.\n",
                "\n",
                "`pipeline` from the `transformers` library and the dataset `wiki` have\n",
                "already been loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a list of text items to summarize from the `wiki` dataset and\n",
                "  save as `text_to_summarize`.\n",
                "- Create a summarization pipeline using a `min_length` of 20 and a\n",
                "  `max_length` of 50 and save as `summarizer`.\n",
                "- Summarize the first three items in the `text_to_summarize` list\n",
                "  setting `truncation` to `True`.\n",
                "- Create a for-loop that will print each text summary.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the list\n",
                "text_to_summarize = [w[\"text\"] for w in wiki]\n",
                "\n",
                "# Create the pipeline\n",
                "summarizer = pipeline(\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=20, max_length=50)\n",
                "\n",
                "# Summarize each item in the list\n",
                "summaries = summarizer(text_to_summarize[:3], truncation=True)\n",
                "\n",
                "# Create for-loop to print each summary\n",
                "for i in range(0,3):\n",
                "  print(f\"Summary {i+1}: {summaries[i]['summary_text']}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building Pipelines for Image and Audio\n",
                "\n",
                "### Processing image data\n",
                "\n",
                "Just like text inputs, image inputs will typically require\n",
                "pre-processing before using with a pipeline for an image-based machine\n",
                "learning task, such as image classification. Some common transformations\n",
                "include cropping and resizing. Fortunately, Hugging Face provides\n",
                "modules for performing these steps via the `image_transforms` module in\n",
                "the `transformers` library.\n",
                "\n",
                "Use this module to apply a transformation to a fashion image.\n",
                "\n",
                "<img\n",
                "src=\"https://assets.datacamp.com/production/repositories/6536/datasets/b8f0d79998622cb33ed09a3c48f1d20b25712bbe/fashion.jpeg\"\n",
                "height=\"500\" />\n",
                "\n",
                "`image_transforms` from the `transformers` library has already been\n",
                "loaded for you, as well as the JPEG saved as `original_image`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Convert the image to a numpy array.\n",
                "- Crop the center of the image to keep a new 200 x 200 image using\n",
                "  `image_transforms` and save as `cropped_image`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the numpy array\n",
                "image_array = np.array(original_image)\n",
                "\n",
                "# Crop the center of the image\n",
                "cropped_image = image_transforms.center_crop(image=image_array, size=(200, 200))\n",
                "\n",
                "imgplot = plt.imshow(cropped_image)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating an image classifier\n",
                "\n",
                "Image classification is the process of labeling an image based on the\n",
                "content. This is useful for many reasons from improving search to saving\n",
                "agriculture crops from disease. It is also helpful for identifying\n",
                "clothing items in fashion photos.\n",
                "\n",
                "Build an image classification pipeline using a model trained on\n",
                "identifying clothing items to classify the image you just cropped.\n",
                "\n",
                "Both `pipeline` from the `transformers` library and the image, saved as\n",
                "`cropped_image`, have already been loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the image classifier pipeline using the model provided and save\n",
                "  as `image_classifier`.\n",
                "- Predict the class of the `cropped_image` and save as `results`.\n",
                "- Print the predicted `\"label\"` of the result.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the pipeline\n",
                "image_classifier = pipeline(task=\"image-classification\", \n",
                "                      model=\"abhishek/autotrain_fashion_mnist_vit_base\")\n",
                "\n",
                "# Predict the class of the image\n",
                "results = image_classifier(cropped_image)\n",
                "\n",
                "# Print the results\n",
                "print(results[0][\"label\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Document question and answering\n",
                "\n",
                "Document question and answering is a multi-modal ML task which analyzes\n",
                "an image of a document, such as a contract, converts it to text, and\n",
                "allows a question to be asked about the text. This is useful when there\n",
                "are many scanned documents which need to be searched, for example\n",
                "financial records.\n",
                "\n",
                "Build a pipeline for document question and answering, then ask the\n",
                "pre-loaded question `Which meeting is this document about?`.\n",
                "\n",
                "`pipeline` from the `transformers` library and the `question` are\n",
                "already loaded for you. Note that we are using our own pipeline and dqa\n",
                "functions to enable you to learn how to use these functions without some\n",
                "of the extra setup. Please visit the Hugging Face documentation to dive\n",
                "deeper.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a pipeline for `document-question-answering` and save as `dqa`.\n",
                "- Save the path to the image, `document.png`, as `image`.\n",
                "- Get the answer for the `question` of the `image` using the `dqa`\n",
                "  pipeline and save as `results`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the pipeline\n",
                "dqa = pipeline(task=\"document-question-answering\", model=\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
                "\n",
                "# Set the image and question\n",
                "image = \"document.png\"\n",
                "question = \"Which meeting is this document about?\"\n",
                "\n",
                "# Get the answer\n",
                "results = dqa(image=image, question=question)\n",
                "\n",
                "print(results)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visual question and answering\n",
                "\n",
                "Visual question and answering is an ML task that attempts to provide the\n",
                "best answer for a question about an image. The model will analyze the\n",
                "content of the image and return a label as the answer.\n",
                "\n",
                "For example, if asking about the clothes a model is wearing, the model\n",
                "may return clothing items as the label. Such a task can be beneficial\n",
                "for people who are visually impaired or as a classification method\n",
                "(similar to image classification but more open-ended).\n",
                "\n",
                "`pipeline` from the `transformers` library and both the `question` and\n",
                "`image` are already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a visual question and answering pipeline by setting the `task`\n",
                "  to `visual-question-answering` and save as `vqa`.\n",
                "- Use the `vqa` pipeline to get an answer for the `image` and\n",
                "  `question`, then save as `results`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pipeline\n",
                "vqa = pipeline(task=\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\")\n",
                "\n",
                "# Use image and question in vqa\n",
                "results = vqa(image=image, question=question)\n",
                "\n",
                "print(results)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Resampling audio files\n",
                "\n",
                "The sampling rate of an audio file determines the resolution. The higher\n",
                "the sampling rate, the higher the resolution which provides more detail\n",
                "about the sound wave itself.\n",
                "\n",
                "When performing ML tasks it is important to ensure each file has the\n",
                "same sampling rate. This will maintain consistency and prepare the audio\n",
                "files based on what the model expects regarding number of data points\n",
                "per audio file.\n",
                "\n",
                "The dataset, `audio_file`, and `Audio` from the `datasets` library are\n",
                "already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Save the old sampling rate, for reference, as `old_sampling_rate`.\n",
                "- Resample the `audio` column to a new rate of 16,000 kHz and save to\n",
                "  `audio_file`.\n",
                "- Compare the old and new sampling rates.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the old sampling rate\n",
                "old_sampling_rate = audio_file[1][\"audio\"][\"sampling_rate\"]\n",
                "\n",
                "# Resample the audio files\n",
                "audio_file = audio_file.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
                "\n",
                "# Compare the old and new sampling rates\n",
                "print(\"Old sampling rate:\", old_sampling_rate)\n",
                "print(\"New sampling rate:\", audio_file[1][\"audio\"][\"sampling_rate\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Filtering out audio files\n",
                "\n",
                "There will be occasions where you will want, or need, to filter a\n",
                "dataset based on a specific criteria. A common example of this is\n",
                "filtering for audio files that are under a specified duration.\n",
                "\n",
                "The `librosa` and `numpy` libraries, as well as the `dataset`, have\n",
                "already been loaded for you. Note: we have modified the `librosa`\n",
                "library for the purposes of this exercise, but the functionality and\n",
                "pattern is the same.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Loop over each `row` of the audio paths in the `dataset` and calculate\n",
                "  the duration, appending to the `old_durations_list`.\n",
                "- Create a new column called `duration` using `old_durations_list` and\n",
                "  save to `dataset`.\n",
                "- Filter the `dataset` for audio under 6.0 seconds using a lambda function and the column `duration`; save as `filtered_dataset`.\n",
                "- Save the new durations as a list called `new_durations_list`.\n",
                "\n",
                "\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a list of durations\n",
                "old_durations_list = []\n",
                "\n",
                "# Loop over the dataset\n",
                "for row in dataset[\"path\"]:\n",
                "    old_durations_list.append(librosa.get_duration(path=row))\n",
                "\n",
                "# Create a new column\n",
                "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
                "\n",
                "\n",
                "# Create a list of durations\n",
                "old_durations_list = []\n",
                "\n",
                "# Loop over the dataset\n",
                "for row in dataset[\"path\"]:\n",
                "    old_durations_list.append(librosa.get_duration(path=row))\n",
                "\n",
                "# Create a new column\n",
                "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
                "\n",
                "# Filter the dataset\n",
                "filtered_dataset = dataset.filter(lambda d: d < 6.0, input_columns=[\"duration\"], keep_in_memory=True)\n",
                "\n",
                "\n",
                "# Create a list of durations\n",
                "old_durations_list = []\n",
                "\n",
                "# Loop over the dataset\n",
                "for row in dataset[\"path\"]:\n",
                "    old_durations_list.append(librosa.get_duration(path=row))\n",
                "\n",
                "# Create a new column\n",
                "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
                "\n",
                "# Filter the dataset\n",
                "filtered_dataset = dataset.filter(lambda d: d < 6.0, input_columns=[\"duration\"], keep_in_memory=True)\n",
                "\n",
                "# Save new durations\n",
                "new_durations_list = filtered_dataset[\"duration\"]\n",
                "\n",
                "print(\"Old duration:\", np.mean(old_durations_list)) \n",
                "print(\"New duration:\", np.mean(new_durations_list))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Classifying audio files\n",
                "\n",
                "Audio classification can be used for any task that requires labeling a\n",
                "piece of audio based on its content. A common use case is identifying\n",
                "spoken languages.\n",
                "\n",
                "You will do just that using an example from the `common_language`\n",
                "dataset. The model, `facebook/mms-lid-126` from Meta is a common model\n",
                "used for this task given its coverage of languages.\n",
                "\n",
                "`pipeline` from the `transformers` library as well as the `dataset` have\n",
                "been loaded for you. It has been modified for the purposes of this\n",
                "exercise.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the pipeline for audio classification and save as `classifier`.\n",
                "- Extract the sample audio and sentence and save as `audio` and\n",
                "  `sentence`, respectively.\n",
                "- Predict the label for the audio using the `classifier` and save as\n",
                "  `prediction`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the pipeline\n",
                "classifier = pipeline(task=\"audio-classification\", model=\"facebook/mms-lid-126\")\n",
                "\n",
                "# Extract the sample\n",
                "audio = dataset[1][\"audio\"][\"array\"]\n",
                "sentence = dataset[1][\"sentence\"]\n",
                "\n",
                "# Predict the language\n",
                "prediction = classifier(audio)\n",
                "\n",
                "print(f\"Predicted language is '{prediction[0]['label'].upper()}' for the sentence '{sentence}'\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Instantiating an ASR pipeline\n",
                "\n",
                "You've been tasked with generating text from a dataset of audio files.\n",
                "Accuracy is important, so you need to make sure you choose the best\n",
                "model for automatic speech recognition. You also don't have the time to\n",
                "train your own model.\n",
                "\n",
                "Compare the predictions between the Wav2Vec and Whisper models by\n",
                "instantiating two pipelines for automatic speech recognition. You want\n",
                "to test out the functionality, so you should try it out on one example\n",
                "first.\n",
                "\n",
                "`pipeline` from the `transformers` package is already loaded for you.\n",
                "Likewise, the dataset has already been loaded for you, resampled, and\n",
                "saved as `english`. One audio file with it's associated metadata is\n",
                "saved as `example`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Instantiate the first automatic speech recognition pipeline for the\n",
                "  \"wav2vec2\" model from Meta.\n",
                "- Predict the text from the `example` audio.\n",
                "- Repeat these two steps for the \"whisper-tiny\" model from OpenAI in\n",
                "  order to compare the predictions.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an ASR pipeline using Meta's wav2vec model\n",
                "meta_asr = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
                "\n",
                "# Predict the text from the example audio\n",
                "meta_pred = meta_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
                "\n",
                "# Repeat for OpenAI's Whisper model\n",
                "open_asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\")\n",
                "open_pred = open_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
                "\n",
                "# Print the prediction from both models\n",
                "print(\"META:\", meta_pred)\n",
                "print(\"OPENAI:\", open_pred)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word error rate\n",
                "\n",
                "The Wav2Vec and Whisper models predicted very similar text with only\n",
                "some minor differences. Luckily, for the first this example record, you\n",
                "have the true sentence for reference. You can use Word Error Rate (WER)\n",
                "to determine which model quantitatively performed the best.\n",
                "\n",
                "`load` from the `evaluate` package has already loaded for you. Likewise,\n",
                "the `example` and predictions - `meta_pred` and `open_pred` - were saved\n",
                "from the previous exercise.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Instantiate the word error rate metric object and save as `wer`.\n",
                "- Save the true sentence of the example as `true_sentence`.\n",
                "- Compute the word error rate for each model prediction and save as\n",
                "  `meta_wer` and `open_wer`, respectively.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the word error rate metric\n",
                "wer = load(\"wer\")\n",
                "\n",
                "# Save the true sentence of the example\n",
                "true_sentence = example[\"sentence\"].lower()\n",
                "\n",
                "# Compute the wer for each model prediction\n",
                "meta_wer = wer.compute(predictions=[meta_pred], references=[true_sentence])\n",
                "open_wer = wer.compute(predictions=[open_pred], references=[true_sentence])\n",
                "\n",
                "print(f\"The WER for the Meta model is {meta_wer} and for the OpenAI model is {open_wer}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Iterating over a dataset\n",
                "\n",
                "You were able test the functionality and understand the performance on\n",
                "one example from the dataset. Now, let's evaluate the models over the\n",
                "first 100 audio files to make a final decision about which is best for\n",
                "this dataset.\n",
                "\n",
                "In order to do this efficiently, you can create a function that will\n",
                "iterate over the rows of the dataset and yield a set of audio and true\n",
                "sentence pairs on each iteration.\n",
                "\n",
                "The dataset, `english`, ASR models - `meta_asr` and `open_asr` - and\n",
                "`pandas` have all been loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the `data()` function to iterate over the first 3 rows of the\n",
                "  dataset.\n",
                "- Within `data()`, predict the text for each audio file using both the\n",
                "  `meta_asr` and `open_asr` pipelines.\n",
                "- Append the results as a dictionary in the `output` list.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the data function\n",
                "def data(n=3):\n",
                "    for i in range(n):\n",
                "        yield english[i][\"audio\"][\"array\"], english[i][\"sentence\"].lower()\n",
                "        \n",
                "# Predict the text for the audio file with both models\n",
                "output = []\n",
                "for audio, sentence in data():\n",
                "    meta_pred = meta_asr(audio)[\"text\"].lower()\n",
                "    open_pred = open_asr(audio)[\"text\"].lower()\n",
                "    # Append to output list\n",
                "    output.append({\"sentence\": sentence, \"metaPred\": meta_pred, \"openPred\": open_pred})\n",
                "\n",
                "output_df = pd.DataFrame(output)\n",
                "\n",
                "\n",
                "# Create the data function\n",
                "def data(n=3):\n",
                "    for i in range(n):\n",
                "        yield english[i][\"audio\"][\"array\"], english[i][\"sentence\"].lower()\n",
                "        \n",
                "# Predict the text for the audio file with both models\n",
                "output = []\n",
                "for audio, sentence in data():\n",
                "    meta_pred = meta_asr(audio)[\"text\"].lower()\n",
                "    open_pred = open_asr(audio)[\"text\"].lower()\n",
                "    # Append to output list\n",
                "    output.append({\"sentence\": sentence, \"metaPred\": meta_pred, \"openPred\": open_pred})\n",
                "\n",
                "output_df = pd.DataFrame(output)\n",
                "\n",
                "# Compute the WER for both models\n",
                "metaWER = wer.compute(predictions=output_df[\"metaPred\"], references=output_df[\"sentence\"])\n",
                "openWER = wer.compute(predictions=output_df[\"openPred\"], references=output_df[\"sentence\"])\n",
                "\n",
                "# Print the WER\n",
                "print(f\"The WER for the meta model is {metaWER} and for the open model is {openWER}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fine-tuning and Embeddings\n",
                "\n",
                "### Preparing a dataset\n",
                "\n",
                "Fine-tuning a model requires several steps including identifying the\n",
                "model to fine-tune, preparing the dataset, creating the training loop\n",
                "object, then saving the model.\n",
                "\n",
                "A model trained on English text classification has been identified for\n",
                "you, but it's up to you to prepare the imdb dataset in order to\n",
                "fine-tune this model to classify the sentiment of movie reviews.\n",
                "\n",
                "The imdb dataset is already loaded for you and saved as `dataset`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `AutoModelForSequenceClassification` and `AutoTokenizer`.\n",
                "- Load the model \"distilbert-base-uncased-finetuned-sst-2-english\" and save as `model`.\n",
                "- Create a tokenizer object based on the model and save as `tokenizer`.\n",
                "- Use `tokenizer` on the `text` in the dataset and save as `dataset`.\n",
                "\n",
                "*Note: parameters are auto-set for the tokenizer and map functions to improve performance for this exercise.*\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import modules\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "\n",
                "\n",
                "# Import modules\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "\n",
                "# Load the model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "# Load the tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "\n",
                "# Import modules\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "\n",
                "# Load the model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "# Load the tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Use tokenizer on text\n",
                "dataset = dataset.map(lambda row: tokenizer(row[\"text\"], padding=True, max_length=512, truncation=True), keep_in_memory=True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Building the trainer\n",
                "\n",
                "To fine-tune a model, it must be trained on new data. This is the\n",
                "process of the model learning patterns within a training dataset, then\n",
                "evaluating how well it can predict patterns in an unseen test dataset.\n",
                "The goal is to help the model build an understanding of patterns while\n",
                "also being generalizable to new data yet to be seen.\n",
                "\n",
                "Build a training object to fine-tune the\n",
                "\"distilbert-base-uncased-finetuned-sst-2-english\" model to be better at\n",
                "identifying sentiment of movie reviews.\n",
                "\n",
                "The `training_data` and `testing_data` dataset are available for you.\n",
                "`Trainer` and `TrainingArguments` from `transformers` are also loaded.\n",
                "They were modified for the purpose of this exercise.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the training arguments object setting the output directory to\n",
                "  `./results`.\n",
                "- Create the trainer object by passing in the model, training arguments,\n",
                "  `training_data` and `testing_data`.\n",
                "- Start the trainer.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create training arguments\n",
                "training_args = TrainingArguments(output_dir=\"./results\")\n",
                "\n",
                "# Create the trainer\n",
                "trainer = Trainer(\n",
                "    model=model, \n",
                "    args=training_args, \n",
                "    train_dataset=training_data, \n",
                "    eval_dataset=testing_data\n",
                ")\n",
                "\n",
                "# Start the trainer\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using the fine-tuned model\n",
                "\n",
                "Now that the model is fine-tuned, it can be used within `pipeline`\n",
                "tasks, such as for sentiment analysis. At this point, the model is\n",
                "typically saved to a local directory (i.e. on your own computer), so a\n",
                "local file path is needed.\n",
                "\n",
                "You'll use the newly fine-tuned distilbert model. There is a sentence,\n",
                "\"I am a HUGE fan of romantic comedies.\", saved as `text_example`.\n",
                "\n",
                "Note: we are using our own `pipeline` module for this exercise for\n",
                "teaching purposes. The model is \"saved\" (i.e. not really) under the path\n",
                "`./fine_tuned_model`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the classifier pipeline for `\"sentiment-analysis\"` and use the\n",
                "  model saved in `\"./fine_tuned_model\"`.\n",
                "- Classify the text, `text_example`, and save the results as `results`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the classifier\n",
                "classifier = pipeline(task=\"sentiment-analysis\", model=\"./fine_tuned_model\")\n",
                "\n",
                "# Classify the text\n",
                "results = classifier(text=text_example)\n",
                "\n",
                "print(results)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generating text from a text prompt\n",
                "\n",
                "Generating text can be accomplished using Auto classes from the Hugging\n",
                "Face `transformers` library. It can be a useful method for developing\n",
                "content, such as technical documentation or creative material.\n",
                "\n",
                "You'll walk through the steps to process the text prompt, \"Wear\n",
                "sunglasses when its sunny because\", then generate new text from it.\n",
                "\n",
                "`AutoTokenizer` and `AutoModelForCausalLM` from the `transformers`\n",
                "library are already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Get the tokenizer and the model for 'gpt2' then save as `tokenizer`\n",
                "  and `model`, respectively.\n",
                "- Tokenize the `prompt` using the `tokenizer` and save as `input_ids`.\n",
                "- Generate new text using the `model` and save as output.\n",
                "- Decode the output and save as `generated_text`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set model name\n",
                "model_name = \"gpt2\"\n",
                "\n",
                "# Get the tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "\n",
                "\n",
                "# Set model name\n",
                "model_name = \"gpt2\"\n",
                "\n",
                "# Get the tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "\n",
                "prompt = \"Wear sunglasses when its sunny because\"\n",
                "\n",
                "# Tokenize the input\n",
                "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
                "\n",
                "\n",
                "# Set model name\n",
                "model_name = \"gpt2\"\n",
                "\n",
                "# Get the tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "\n",
                "prompt = \"Wear sunglasses when its sunny because\"\n",
                "\n",
                "# Tokenize the input\n",
                "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
                "\n",
                "# Generate the text output\n",
                "output = model.generate(input_ids, num_return_sequences=1)\n",
                "\n",
                "# Decode the output\n",
                "generated_text = tokenizer.decode(output[0])\n",
                "\n",
                "print(generated_text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generating a caption for an image\n",
                "\n",
                "Generating text can be done for modalities other than text, such as\n",
                "images. This has a lot of benefits including faster content creation by\n",
                "generating captions from images.\n",
                "\n",
                "You'll create a caption for a fashion image using the Microsoft GIT\n",
                "model (\"microsoft/git-base-coco\").\n",
                "\n",
                "`AutoProcessor` and `AutoModelForCausalLM` from the `transformers`\n",
                "library is already loaded for you along with the `image`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Get the processor and model and save with the same names,\n",
                "  respectively.\n",
                "- Process the `image` using the processor and save as `pixels`.\n",
                "- Generate the output and save as `output`.\n",
                "- Decode the `output` and save as `caption`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the processor and model\n",
                "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
                "\n",
                "# Process the image\n",
                "pixels = processor(images=image, return_tensors=\"pt\").pixel_values\n",
                "\n",
                "# Generate the ids\n",
                "output = model.generate(pixel_values=pixels)\n",
                "\n",
                "# Decode the output\n",
                "caption = processor.batch_decode(output)\n",
                "\n",
                "print(caption[0])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generate embeddings for a sentence\n",
                "\n",
                "Embeddings are playing an increasingly big role in ML and AI systems. A\n",
                "common use case is embedding text to support search.\n",
                "\n",
                "The `sentence-transformers` package from Hugging Face is useful for\n",
                "getting started with embedding models. You'll compare the embedding\n",
                "shape from two different models - \"all-MiniLM-L6-v2\" and\n",
                "\"sentence-transformers/paraphrase-albert-small-v2\". This can determine\n",
                "which is better suited for a project (i.e. because of storage\n",
                "constraints).\n",
                "\n",
                "The sentence used for embedding, *\"Programmers, do you put your comments\n",
                "(before\\|after) the related code?\"*, is saved as `sentence`.\n",
                "\n",
                "`SentenceTransformer` from the `sentence-transformers` package was\n",
                "already loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the first embedder using `SentenceTransformer` and the model\n",
                "  `all-MiniLM-L6-v2`, then save as `embedder1`.\n",
                "- Use `embedder1` to generate an embedding for the `sentence` and save\n",
                "  as `embedding1`.\n",
                "- Repeat these two steps for the `paraphrase-albert-small-v2` model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the first embedding model\n",
                "embedder1 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# Embed the sentence\n",
                "embedding1 = embedder1.encode([sentence])\n",
                "\n",
                "# Create and use second embedding model\n",
                "embedder2 = SentenceTransformer(\"sentence-transformers/paraphrase-albert-small-v2\")\n",
                "embedding2 = embedder2.encode([sentence])\n",
                " \n",
                "# Compare the shapes\n",
                "print(embedding1.shape == embedding2.shape)\n",
                "\n",
                "# Print embedding1\n",
                "print(embedding1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using semantic search\n",
                "\n",
                "The similarity, or closeness, between a query and the other sentences,\n",
                "or documents, is the foundation for semantic search. This is a search\n",
                "method which takes into account context and intent of the query.\n",
                "Similarity measures, such as cosine similarity, are used to quantify the\n",
                "distance between the query and each sentence within the dimensional\n",
                "space. Results of a search are based on the closest sentences to the\n",
                "query.\n",
                "\n",
                "You will use semantic search to return the top two Reddit threads\n",
                "relevant to the user query, \"I need a desktop book reader for Mac\".\n",
                "\n",
                "The `embedder` and `sentence_embeddings` are already loaded for you\n",
                "along with `util.semantic_search()`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Generate embeddings for the query and save as `query_embedding`.\n",
                "- Use `util.semantic_search` to compare the query embedding with the\n",
                "  sentence embeddings and save as `hits`.\n",
                "- Complete the `for-loop` to print the results from `hits[0]`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"I need a desktop book reader for Mac\"\n",
                "\n",
                "# Generate embeddings\n",
                "query_embedding = embedder.encode([query])[0]\n",
                "\n",
                "# Compare embeddings\n",
                "hits = util.semantic_search(query_embedding, sentence_embeddings, top_k=2)\n",
                "\n",
                "# Print the top results\n",
                "for hit in hits[0]:\n",
                "    print(sentences[hit[\"corpus_id\"]], \"(Score: {:.4f})\".format(hit[\"score\"]))\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
